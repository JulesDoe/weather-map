{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folder Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 26450\n",
      "0  |  1000  |  2000  |  3000  |  4000  |  5000  |  6000  |  7000  |  8000  |  9000  |  10000  |  11000  |  12000  |  13000  |  14000  |  15000  |  16000  |  17000  |  18000  |  19000  |  20000  |  21000  |  22000  |  23000  |  24000  |  25000  |  26000  |  "
     ]
    }
   ],
   "source": [
    "# The index of these four arrays corresponds to one organization\n",
    "\n",
    "orgs = []\n",
    "texts = []\n",
    "years = []\n",
    "occurences = []\n",
    "urls = []\n",
    "lengths = []\n",
    "\n",
    "\n",
    "# The template counts the occurencies for each organization\n",
    "\n",
    "years_template = { 2011: 0, 2012: 0, 2013: 0, 2014: 0, 2015: 0, 2016: 0, 2017: 0, 2018: 0, 2019: 0, 2020: 0 }\n",
    "\n",
    "\n",
    "# Folder\n",
    "\n",
    "dir = 'data/biomass/'\n",
    "files = os.listdir(dir)\n",
    "print('Total', len(files))\n",
    "\n",
    "for index, filename in enumerate(files):\n",
    "\n",
    "    # Read file\n",
    "    \n",
    "    f = open(dir + filename)\n",
    "    r = f.read()\n",
    "    data = json.loads(r)\n",
    "\n",
    "\n",
    "    # Set basic metadata\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Counter\n",
    "\n",
    "        if not index % 1000:\n",
    "            print(index, ' | ', end=' ')\n",
    "\n",
    "        # Collect information\n",
    "\n",
    "        url = data[0]['url']\n",
    "        records = data[0]['story_tags'] # Set tags\n",
    "        length = len(records)\n",
    "        year = int(data[0]['publish_date'].split(' ')[0].split('-')[0]) # Set year\n",
    "\n",
    "        # Allow list\n",
    "        \n",
    "        matches = ['wsj.com', 'usatoday.com', 'nytimes.com', 'latimes.com', 'nypost.com', 'washingtonpost.com', 'startribune.com', 'chicagotribune.com', 'chron.com', 'nydailynews.com']\n",
    "        if not any(x in url for x in matches): continue\n",
    "\n",
    "        # Stop list (Bob Dylan case)\n",
    "        \n",
    "        matches = {'feeds.latimes.com'}\n",
    "        if any(x in url for x in matches): continue\n",
    "\n",
    "        # Max tags\n",
    "        if  length > 100: continue # Limit the maxiumum number of records\n",
    "        \n",
    "        # Save\n",
    "        \n",
    "        urls.append(url)\n",
    "        lengths.append(length)\n",
    "        \n",
    "    \n",
    "    except:\n",
    "        continue\n",
    "\n",
    "\n",
    "    # Collect entities\n",
    "    \n",
    "    tags = []\n",
    "\n",
    "    for record in records:\n",
    "        \n",
    "        # Filter tags\n",
    "        \n",
    "        matches = ['nyt_labels', 'cliff_organizations', 'cliff_people']\n",
    "        tag_set = record['tag_set']\n",
    "\n",
    "        if any(x in tag_set for x in matches):\n",
    "        # if (record['tag_set'] == 'cliff_people'):\n",
    "            \n",
    "            tag = record['tag']\n",
    "\n",
    "            # if len(tag) > 50:\n",
    "            #     continue # Set limits to tag's length\n",
    "\n",
    "            # if len(tag) < 4:\n",
    "            #     continue # Set limits to tag's length\n",
    "\n",
    "            tag = ' '.join(tag.split()) # Merge spaces\n",
    "            tag = tag.replace('.', '')\n",
    "            tag = tag.replace(',', '')\n",
    "            \n",
    "            if tag.startswith('US '): tag = tag[3:]\n",
    "            if tag.endswith(' Corp'): tag = tag[:-5]\n",
    "            if tag.endswith(' Inc'): tag = tag[:-4]\n",
    "            \n",
    "            \n",
    "            # if tag[0].isupper() and tag[1].isupper():\n",
    "            #     continue # Remove acronyms\n",
    "            \n",
    "            if tag[0] == ('—'):\n",
    "                continue # Remove listing\n",
    "\n",
    "            tags.append(tag)\n",
    "    \n",
    "    \n",
    "    # Tgas cleaning\n",
    "\n",
    "    stoplist = {'nyt_labeller_v100', 'no index terms from nytimes', 'post', 'Post', 'Google', 'Facebook', 'Amazon', 'YouTube', 'brown'}\n",
    "\n",
    "    tags = list(filter(lambda x: not x in stoplist, tags))\n",
    "\n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag == 'Xcel': tags[i] = 'Xcel Energy'\n",
    "\n",
    "        if tag == 'Obama': tags[i] = 'Barack Obama'\n",
    "        if tag == 'Trump': tags[i] = 'Donald Trump'\n",
    "        if tag == 'USA TODAY': tags[i] = 'USA Today'\n",
    "\n",
    "        if tag == 'NOAA': tags[i] = 'National Oceanic and Atmospheric Administration'\n",
    "        if tag == 'EPA': tags[i] = 'Environmental Protection Agency'\n",
    "        if tag == 'United States Environmental Protection Agency': tags[i] = 'Environmental Protection Agency'\n",
    "        if tag == 'UN': tags[i] = 'United Nations'\n",
    "        if tag == 'EC': tags[i] = 'European Commission'\n",
    "        if tag == 'IEA': tags[i] = 'International Energy Agency'\n",
    "        if tag == 'EU': tags[i] = 'European Union'\n",
    "        if tag == 'EIA': tags[i] = 'Energy Information Administration'\n",
    "        if tag == 'AP': tags[i] = 'Associated Press'\n",
    "        if tag == 'ASSOCIATED PRESS': tags[i] = 'Associated Press'\n",
    "        if tag == 'DOE': tags[i] = 'Department of Energy'\n",
    "        if tag == 'Energy': tags[i] = 'Department of Energy'\n",
    "        if tag == 'Energy Department': tags[i] = 'Department of Energy'\n",
    "        if tag == 'GM': tags[i] = 'General Motors'\n",
    "        if tag == 'IRS': tags[i] = 'Internal Revenue Service'\n",
    "        if tag == 'GOP': tags[i] = 'Republican Party'\n",
    "        if tag == 'USDA': tags[i] = 'Department of Agriculture'\n",
    "        if tag == 'Agriculture Department': tags[i] = 'Department of Agriculture'\n",
    "        if tag == 'TPWD': tags[i] = 'Texas Parks and Wildlife Department'\n",
    "        if tag == 'DNR': tags[i] = 'Department of Natural Resources'\n",
    "        if tag == 'FDA': tags[i] = 'Food and Drug Administration'\n",
    "        if tag == 'OPEC': tags[i] = 'Organization of the Petroleum Exporting Countries'\n",
    "        \n",
    "        if tag == 'Stanford': tags[i] = 'Stanford University'\n",
    "        if tag == 'Harvard': tags[i] = 'Harvard University'\n",
    "        if tag == 'Yale': tags[i] = 'Yale University'\n",
    "        if tag == 'Nasa': tags[i] = 'NASA'\n",
    "        if tag == 'MSP': tags[i] = 'Minneapolis–Saint Paul'\n",
    "        if tag == 'St Paul': tags[i] = 'Minneapolis–Saint Paul'\n",
    "\n",
    "        if tag.startswith('University of California'): tags[i] = 'University of California'\n",
    "        if tag.startswith('Columbia University'): tags[i] = 'Columbia University'\n",
    "\n",
    "        if tag.startswith('Bloomberg'): tags[i] = 'Bloomberg'\n",
    "        if tag.startswith('UBS'): tags[i] = 'UBS'\n",
    "        if tag.startswith('Exxon'): tags[i] = 'Exxon'\n",
    "        if tag.startswith('Drax'): tags[i] = 'Drax'\n",
    "        if tag.startswith('Valens'): tags[i] = 'Valens'\n",
    "        if tag.startswith('Siemens'): tags[i] = 'Siemens'\n",
    "        if tag.startswith('Abengoa'): tags[i] = 'Abengoa'\n",
    "        if tag.startswith('BBC'): tags[i] = 'BBC'\n",
    "        if tag.startswith('New York Times'): tags[i] = 'New York Times'\n",
    "\n",
    "        if tag.startswith('Sierra Club'): tags[i] = 'Sierra Club'\n",
    "        if tag == 'Sierra': tags[i] = 'Sierra Club'\n",
    "\n",
    "\n",
    "    tags = list(set(tags))\n",
    "    # print(tags)\n",
    "\n",
    "\n",
    "    # Create data structure\n",
    "\n",
    "    for t in tags:\n",
    "\n",
    "        related = tags.copy()\n",
    "        related.remove(t)    \n",
    "    \n",
    "        if t not in orgs:\n",
    "            orgs.append(t)\n",
    "            i = orgs.index(t)\n",
    "            texts.append(related)\n",
    "            occurences.append(1)\n",
    "            years.append(years_template.copy())\n",
    "            years[i][year] += 1\n",
    "                \n",
    "        else:\n",
    "            i = orgs.index(t)\n",
    "            texts[i] += related\n",
    "            occurences[i] += 1\n",
    "            years[i][year] += 1\n",
    "\n",
    "    # if index > 20:\n",
    "    #     raise SystemExit(\"Stop right there!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       orgs: 4905 \n",
      "      texts: 4905 \n",
      "      years: 4905 \n",
      " occurences: 4905 \n",
      "lengths: 568 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "copy_orgs = orgs.copy()\n",
    "copy_texts = texts.copy()\n",
    "copy_years = years.copy()\n",
    "copy_occurences = occurences.copy()\n",
    "\n",
    "# Print\n",
    "\n",
    "print(\n",
    "    '       orgs:', len(orgs), '\\n',\n",
    "    '     texts:', len(texts), '\\n', \n",
    "    '     years:', len(years), '\\n', \n",
    "    'occurences:', len(occurences), '\\n'\n",
    "    'lengths:', len(lengths), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = orgs.index('Google')\n",
    "# print(orgs[n], '\\n')\n",
    "# print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CVS Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('orgs.csv', 'w', encoding='UTF8') as f:\n",
    "    for n in orgs:\n",
    "        i = orgs.index(n)\n",
    "        f.write(n + \",\" + str(occurences[i]) + '\\n')\n",
    "\n",
    "with open('urls.csv', 'w', encoding='UTF8') as f:\n",
    "    for n in urls:\n",
    "        i = urls.index(n)\n",
    "        f.write(str(urls[i]) + '\\n')\n",
    "\n",
    "with open('lengths.csv', 'w', encoding='UTF8') as f:\n",
    "    for n in lengths:\n",
    "        i = lengths.index(n)\n",
    "        f.write(str(lengths[i]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       orgs: 162 \n",
      "      texts: 162 \n",
      "      years: 162 \n",
      " occurences: 162 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "orgs = copy_orgs.copy()\n",
    "texts = copy_texts.copy()\n",
    "years = copy_years.copy()\n",
    "occurences = copy_occurences.copy()\n",
    "\n",
    "\n",
    "# Set occurrence limit\n",
    "\n",
    "for index, occurrence in reversed(list(enumerate(occurences))):\n",
    "\n",
    "    min = 5\n",
    "\n",
    "    if occurrence < min:\n",
    "        orgs.pop(index)\n",
    "        texts.pop(index)\n",
    "        years.pop(index)\n",
    "        occurences.pop(index)\n",
    "\n",
    "\n",
    "# Order years by key in tuples\n",
    "\n",
    "for index, y in enumerate(years):\n",
    "    sortedDict = dict( sorted(y.items(), key=lambda x: x[0]) )\n",
    "    _temp = {}\n",
    "    for k,v in sortedDict.items():\n",
    "        _temp[k] = v\n",
    "    years[index] = _temp\n",
    "\n",
    "\n",
    "# Print\n",
    "\n",
    "print(\n",
    "    '       orgs:', len(orgs), '\\n',\n",
    "    '     texts:', len(texts), '\\n', \n",
    "    '     years:', len(years), '\\n', \n",
    "    'occurences:', len(occurences), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This should be on the number of articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({2011: 423,\n",
       "  2012: 372,\n",
       "  2013: 328,\n",
       "  2014: 313,\n",
       "  2015: 451,\n",
       "  2016: 457,\n",
       "  2017: 313,\n",
       "  2018: 364,\n",
       "  2019: 342,\n",
       "  2020: 263},\n",
       " -0.13939393939393935)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total linear regression\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "total_years = {}\n",
    "\n",
    "for year in years:\n",
    "    # print(year)\n",
    "    for k, v in year.items():\n",
    "        # print(k, v)\n",
    "        if k in total_years:\n",
    "            total_years[k] += v\n",
    "        else:\n",
    "            total_years[k] = v\n",
    "\n",
    "y = list(year.values())\n",
    "x = list(year.keys())\n",
    "x = np.array(x).reshape((-1, 1))\n",
    "\n",
    "model = LinearRegression().fit(x, y)\n",
    "slope = model.coef_\n",
    "total_slope = slope[0]\n",
    "# score = model.score(x, y)\n",
    "\n",
    "total_years, total_slope\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min -1.4121212121212114 max 1.236363636363636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-141-50bf3f7e9c14>:41: MatplotlibDeprecationWarning: \n",
      "The DivergingNorm class was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use TwoSlopeNorm instead.\n",
      "  norm = matplotlib.colors.DivergingNorm(vmin=_min, vcenter=0, vmax=_max)\n"
     ]
    }
   ],
   "source": [
    "# Linear regression\n",
    "\n",
    "import matplotlib.colors\n",
    "\n",
    "slopes = []\n",
    "colors = []\n",
    "\n",
    "_min = 0\n",
    "_max = 0\n",
    "\n",
    "\n",
    "# Slope\n",
    "\n",
    "for year in years:\n",
    "\n",
    "    y = list(year.values())\n",
    "    x = list(year.keys())\n",
    "    x = np.array(x).reshape((-1, 1))\n",
    "    \n",
    "    model = LinearRegression().fit(x, y)\n",
    "    slope = model.coef_\n",
    "    slope = slope[0] - total_slope\n",
    "    # slope = slope[0]\n",
    "    score = model.score(x, y)\n",
    "    slopes.append(slope)\n",
    "\n",
    "    if slope > _max: _max = slope\n",
    "    if slope < _min: _min = slope\n",
    "\n",
    "    # print()\n",
    "    # print(list(year.keys()), y)\n",
    "    # print('slope', slope, 'score', score)\n",
    "\n",
    "print('min', _min, 'max', _max)\n",
    "\n",
    "# Colors\n",
    "\n",
    "cmap = plt.cm.RdYlBu_r\n",
    "cmap = plt.cm.coolwarm\n",
    "# norm = matplotlib.colors.Normalize(vmin=_min, vmax=_max)\n",
    "norm = matplotlib.colors.DivergingNorm(vmin=_min, vcenter=0, vmax=_max)\n",
    "# norm = matplotlib.colors.DivergingNorm(vmin=-10, vcenter=0, vmax=10)\n",
    "\n",
    "for slope in slopes:\n",
    "    color = cmap(norm(slope))\n",
    "    colors.append(color)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term Frequency Matrix\n",
    "\n",
    "import textacy\n",
    "\n",
    "doc_term_matrix, dictionary = textacy.representations.build_doc_term_matrix(texts, tf_type=\"linear\", idf_type=\"smooth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP\n",
    "\n",
    "import umap\n",
    "from pointgrid import align_points_to_grid\n",
    "\n",
    "# random_state=2\n",
    "reducer = umap.UMAP(n_components=2, n_neighbors=3, min_dist=0.01, metric='cosine')\n",
    "# reducer = umap.UMAP(n_components=2, n_neighbors=2, metric='hellinger')\n",
    "\n",
    "embedding = reducer.fit_transform(doc_term_matrix)\n",
    "# embedding = align_points_to_grid(embedding)\n",
    "x = embedding[:, 0]; y = embedding[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clustering on embedding\n",
    "\n",
    "import hdbscan\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=4, min_samples=3, cluster_selection_epsilon=.5)\n",
    "# clusterer = hdbscan.HDBSCAN(min_cluster_size=2, min_samples=3)\n",
    "# clusterer = hdbscan.HDBSCAN(cluster_selection_epsilon=0.3, cluster_selection_method='leaf')\n",
    "# min_samples is to consier all the elements that owtherwide will be classified as noise\n",
    "# cluster_selection_epsilon extends clusters\n",
    "clusterer.fit(embedding)\n",
    "clusters = clusterer.labels_\n",
    "\n",
    "# Grouping by cluster\n",
    "\n",
    "values = set(clusters)\n",
    "if -1 in values: values.remove(-1)\n",
    "\n",
    "clusters = [[index for index, cluster in enumerate(clusters) if cluster==value] for value in values]\n",
    "\n",
    "len(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy import interpolate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Frame\n",
    "\n",
    "plt.figure(figsize=(20,20), dpi=300)\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "# Hulls\n",
    "\n",
    "for cluster in clusters:\n",
    "\n",
    "    # Average color\n",
    "    \n",
    "    background_color = []\n",
    "    \n",
    "    for i, index in enumerate(cluster):\n",
    "        for occurence in range(occurences[index]):\n",
    "            background_color.append([colors[index][0], colors[index][1], colors[index][2]])\n",
    "\n",
    "    r = [i[0] for i in background_color]; r = sum(r) / len(r)\n",
    "    g = [i[1] for i in background_color]; g = sum(g) / len(g)\n",
    "    b = [i[2] for i in background_color]; b = sum(b) / len(b)\n",
    "\n",
    "    background_color = (r, g, b, 1)\n",
    "\n",
    "    # Hull\n",
    "\n",
    "    points = []\n",
    "    for index in cluster:\n",
    "        points.append([embedding[index][0], embedding[index][1]])\n",
    "    points = np.array(points)\n",
    "\n",
    "    # print(points)\n",
    "\n",
    "    hull = ConvexHull(points)\n",
    "    \n",
    "    x_hull = np.append(points[hull.vertices,0], points[hull.vertices,0][0]) # Collect the xs + first x\n",
    "    y_hull = np.append(points[hull.vertices,1], points[hull.vertices,1][0])\n",
    "\n",
    "    # print(x_hull)\n",
    "\n",
    "    # break\n",
    "    \n",
    "    # interpolate\n",
    "    dist = np.sqrt((x_hull[:-1] - x_hull[1:])**2 + (y_hull[:-1] - y_hull[1:])**2)\n",
    "    dist_along = np.concatenate(([0], dist.cumsum()))\n",
    "    spline, u = interpolate.splprep([x_hull, y_hull], u=dist_along, s=0)\n",
    "    interp_d = np.linspace(dist_along[0], dist_along[-1], 50)\n",
    "    interp_x, interp_y = interpolate.splev(interp_d, spline)\n",
    "    \n",
    "    # plot shape\n",
    "    plt.fill(interp_x, interp_y, '--', c=background_color, alpha=.2)\n",
    "\n",
    "\n",
    "# Scatterplot\n",
    "\n",
    "plt.scatter(x, y, s=occurences, c=colors)\n",
    "# plt.scatter(x, y, s=40, c=colors)\n",
    "\n",
    "\n",
    "# Labels\n",
    "\n",
    "for i, txt in enumerate(orgs):\n",
    "    # text = plt.annotate(orgs[i], xy=(x[i], y[i] - math.sqrt(occurences[i]/math.pi)/40), ha='center', va='bottom')\n",
    "    text = plt.annotate(orgs[i], xy=(x[i], y[i]), ha='center', va='bottom')\n",
    "    text.set_fontsize(3)\n",
    "\n",
    "plt.savefig('/Users/dario/Desktop/download.png')\n",
    "plt.savefig('download.png')\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c30f2af5f468e7f5b45bcc30fca5f4886c90d54777aed916ed5f6294dfb24bf2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
