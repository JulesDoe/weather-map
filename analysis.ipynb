{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folder Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 26450\n",
      "11000  |  "
     ]
    }
   ],
   "source": [
    "# Reading the whole folder\n",
    "\n",
    "entities = []\n",
    "\n",
    "\n",
    "# The index of these four arrays corresponds to one organization\n",
    "\n",
    "orgs = []\n",
    "texts = []\n",
    "years = []\n",
    "occurences = []\n",
    "urls = []\n",
    "\n",
    "\n",
    "# The template counts the occurencies for each organization\n",
    "\n",
    "years_template = { 2011: 0, 2012: 0, 2013: 0, 2014: 0, 2015: 0, 2016: 0, 2017: 0, 2018: 0, 2019: 0, 2020: 0 }\n",
    "\n",
    "\n",
    "# Folder\n",
    "\n",
    "dir = 'data/biomass/'\n",
    "files = os.listdir(dir)\n",
    "print('Total', len(files))\n",
    "\n",
    "for index, filename in enumerate(files):\n",
    "\n",
    "    # Read file\n",
    "    \n",
    "    f = open(dir + filename)\n",
    "    r = f.read()\n",
    "    data = json.loads(r)\n",
    "\n",
    "\n",
    "    # Set basic metadata\n",
    "\n",
    "    try:\n",
    "\n",
    "        matches = ['wsj.com', 'usatoday.com', 'nytimes.com', 'latimes.com', 'nypost.com', 'washingtonpost.com', 'startribune.com', 'chicagotribune.com', 'chron.com', 'nydailynews.com']\n",
    "        \n",
    "        # matches = ['wsj.com']\n",
    "        url = data[0]['guid']\n",
    "        if not any(x in url for x in matches):\n",
    "            continue\n",
    "        \n",
    "        records = data[0]['story_tags'] # Set tags\n",
    "        year = int(data[0]['publish_date'].split(' ')[0].split('-')[0]) # Set year\n",
    "        if len(records) > 200: continue # Limit the maxiumum number of records\n",
    "        \n",
    "        if not index % 1000:\n",
    "            print(index, ' | ', end=' ')\n",
    "\n",
    "        urls.append(url)\n",
    "    \n",
    "    except:\n",
    "        continue\n",
    "\n",
    "\n",
    "    # Collect entities\n",
    "    \n",
    "    tags = []\n",
    "\n",
    "    for record in records:\n",
    "        \n",
    "        # Filter tags\n",
    "        \n",
    "        matches = ['nyt_labels', 'cliff_organizations', 'cliff_people']\n",
    "        tag_set = record['tag_set']\n",
    "\n",
    "        if any(x in tag_set for x in matches):\n",
    "        # if (record['tag_set'] == 'cliff_people'):\n",
    "            \n",
    "            tag = record['tag']\n",
    "\n",
    "            # if len(tag) > 50:\n",
    "            #     continue\n",
    "\n",
    "            # if len(tag) < 4:\n",
    "            #     continue\n",
    "\n",
    "            # if tag[0].islower():\n",
    "            #     continue # If the tag starts with lowercase\n",
    "\n",
    "            # if tag[0].isupper() and tag[1].isupper():\n",
    "            #     continue # Remove acronyms\n",
    "            \n",
    "            tag = tag.replace('.', '')\n",
    "            tag = tag.replace(',', '')\n",
    "            # tag = tag.replace(', Inc', '')\n",
    "            # tag = tag.replace(' Co', '')\n",
    "            # tag = tag.replace('US ', '')\n",
    "            # tag = tag.replace('mmission', ' Commission')\n",
    "            # tag = tag.replace('llege', ' College')\n",
    "            # tag = tag.replace('mmerce', ' Commerce')\n",
    "            # tag = tag.replace('lumbia', ' Columbia')\n",
    "            # tag = tag.replace('ourt', ' Court')\n",
    "            # tag = tag.replace('rporation', ' Corporation')\n",
    "            # tag = tag.replace('eurt', ' Coeurt')\n",
    "\n",
    "            # if tag == 'ExxonMobil': tag = 'Exxon'\n",
    "            # if tag == 'Exxon Mobil': tag = 'Exxon'\n",
    "            # if tag == 'Interior': tag = 'Interior Department'\n",
    "            # if tag == 'Reuters': tag = 'Thomson Reuters'\n",
    "            # if tag == 'Royal Dutch Shell': tag = 'Shell'\n",
    "            # if tag == 'World Health Organisation': tag = 'World Health Organization'\n",
    "            # if tag == 'International Monetary Found': tag = 'International Monetary Fund'\n",
    "            # if tag == 'European commission': tag = 'European Commission'\n",
    "            # if tag == 'Department of Agriculture': tag = 'Agriculture Department'\n",
    "            # if tag == 'Department of Energy': tag = 'Energy Department'\n",
    "            # if tag == 'State': tag = 'State Department'\n",
    "            # if tag == 'House': tag = 'White House'\n",
    "            # if tag == 'Sierra': tag = 'Sierra Club'\n",
    "            \n",
    "            # if tag == 'Yale': tag = 'Yale University'\n",
    "            # if tag == 'Penn State': tag = 'Pennsylvania State University'\n",
    "            # if tag == 'Oxford of University': tag = 'University Oxford'\n",
    "            # if tag == 'Oxford': tag = 'Oxford University'\n",
    "            # if tag == 'Stanford': tag = 'Stanford University'\n",
    "            # if tag == 'Massachusetts Institute of Technology': tag = 'MIT'\n",
    "            # if tag == 'Harvard': tag = 'Harvard University'\n",
    "            # if tag == 'University of California, Berkeley': tag = 'University of California'\n",
    "\n",
    "            # if tag.startswith('Valens'): tag = 'Valens'\n",
    "            # if tag.startswith('Siemens'): tag = 'Siemens'\n",
    "            # if tag.startswith('Bloomberg'): tag = 'Bloomberg'\n",
    "            # if tag.startswith('Drax'): tag = 'Drax'\n",
    "\n",
    "            # if ('University' or 'College' or 'Institute') in tag:\n",
    "            #     continue\n",
    "\n",
    "            # stoplist = {'Inc', 'Amazon rainforest', 'Agriculture', 'Energy', 'Ltd', 'Republican', 'MIT', 'Times', 'Parliament', 'UN', 'Liverpool'}\n",
    "            \n",
    "            # if tag not in stoplist:\n",
    "            tags.append(tag)\n",
    "    \n",
    "    \n",
    "    # Create data structure\n",
    "\n",
    "    for t in tags:\n",
    "\n",
    "        related = tags.copy()\n",
    "        related.remove(t)    \n",
    "    \n",
    "        if t not in orgs:\n",
    "            orgs.append(t)\n",
    "            i = orgs.index(t)\n",
    "            texts.append(related)\n",
    "            occurences.append(1)\n",
    "            years.append(years_template.copy())\n",
    "            years[i][year] += 1\n",
    "                \n",
    "        else:\n",
    "            i = orgs.index(t)\n",
    "            texts[i] += related\n",
    "            occurences[i] += 1\n",
    "            years[i][year] += 1\n",
    "\n",
    "    # if index > 10:\n",
    "    #     raise SystemExit(\"Stop right there!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       orgs: 5563 \n",
      "      texts: 5563 \n",
      "      years: 5563 \n",
      " occurences: 5563 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "copy_orgs = orgs.copy()\n",
    "copy_texts = texts.copy()\n",
    "copy_years = years.copy()\n",
    "copy_occurences = occurences.copy()\n",
    "\n",
    "# Print\n",
    "\n",
    "print(\n",
    "    '       orgs:', len(orgs), '\\n',\n",
    "    '     texts:', len(texts), '\\n', \n",
    "    '     years:', len(years), '\\n', \n",
    "    'occurences:', len(occurences), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = orgs.index('Google')\n",
    "# print(orgs[n], '\\n')\n",
    "# print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CVS Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "pairs = []\n",
    "with open('orgs.csv', 'w', encoding='UTF8') as f:\n",
    "    for n in orgs:\n",
    "        i = orgs.index(n)\n",
    "        f.write(n + \",\" + str(occurences[i]) + '\\n')\n",
    "\n",
    "with open('urls.csv', 'w', encoding='UTF8') as f:\n",
    "    for n in urls:\n",
    "        i = urls.index(n)\n",
    "        f.write(str(urls[i]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       orgs: 157 \n",
      "      texts: 157 \n",
      "      years: 157 \n",
      " occurences: 157 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "orgs = copy_orgs.copy()\n",
    "texts = copy_texts.copy()\n",
    "years = copy_years.copy()\n",
    "occurences = copy_occurences.copy()\n",
    "\n",
    "\n",
    "# Set occurrence limit\n",
    "\n",
    "for index, occurrence in reversed(list(enumerate(occurences))):\n",
    "\n",
    "    min = 10\n",
    "\n",
    "    if occurrence < min:\n",
    "        orgs.pop(index)\n",
    "        texts.pop(index)\n",
    "        years.pop(index)\n",
    "        occurences.pop(index)\n",
    "\n",
    "\n",
    "# Order years by key in tuples\n",
    "\n",
    "for index, y in enumerate(years):\n",
    "    sortedDict = dict( sorted(y.items(), key=lambda x: x[0]) )\n",
    "    _temp = {}\n",
    "    for k,v in sortedDict.items():\n",
    "        _temp[k] = v\n",
    "    years[index] = _temp\n",
    "\n",
    "\n",
    "# Print\n",
    "\n",
    "print(\n",
    "    '       orgs:', len(orgs), '\\n',\n",
    "    '     texts:', len(texts), '\\n', \n",
    "    '     years:', len(years), '\\n', \n",
    "    'occurences:', len(occurences), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This should be on the number of articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({2011: 474,\n",
       "  2012: 386,\n",
       "  2013: 339,\n",
       "  2014: 323,\n",
       "  2015: 969,\n",
       "  2016: 1419,\n",
       "  2017: 332,\n",
       "  2018: 340,\n",
       "  2019: 328,\n",
       "  2020: 240},\n",
       " 0.06666666666666665)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total linear regression\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "total_years = {}\n",
    "\n",
    "for year in years:\n",
    "    # print(year)\n",
    "    for k, v in year.items():\n",
    "        # print(k, v)\n",
    "        if k in total_years:\n",
    "            total_years[k] += v\n",
    "        else:\n",
    "            total_years[k] = v\n",
    "\n",
    "y = list(year.values())\n",
    "x = list(year.keys())\n",
    "x = np.array(x).reshape((-1, 1))\n",
    "\n",
    "model = LinearRegression().fit(x, y)\n",
    "slope = model.coef_\n",
    "total_slope = slope[0]\n",
    "# score = model.score(x, y)\n",
    "\n",
    "total_years, total_slope\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min -2.593939393939393 max 0.8909090909090905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-147-50bf3f7e9c14>:41: MatplotlibDeprecationWarning: \n",
      "The DivergingNorm class was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use TwoSlopeNorm instead.\n",
      "  norm = matplotlib.colors.DivergingNorm(vmin=_min, vcenter=0, vmax=_max)\n"
     ]
    }
   ],
   "source": [
    "# Linear regression\n",
    "\n",
    "import matplotlib.colors\n",
    "\n",
    "slopes = []\n",
    "colors = []\n",
    "\n",
    "_min = 0\n",
    "_max = 0\n",
    "\n",
    "\n",
    "# Slope\n",
    "\n",
    "for year in years:\n",
    "\n",
    "    y = list(year.values())\n",
    "    x = list(year.keys())\n",
    "    x = np.array(x).reshape((-1, 1))\n",
    "    \n",
    "    model = LinearRegression().fit(x, y)\n",
    "    slope = model.coef_\n",
    "    slope = slope[0] - total_slope\n",
    "    # slope = slope[0]\n",
    "    score = model.score(x, y)\n",
    "    slopes.append(slope)\n",
    "\n",
    "    if slope > _max: _max = slope\n",
    "    if slope < _min: _min = slope\n",
    "\n",
    "    # print()\n",
    "    # print(list(year.keys()), y)\n",
    "    # print('slope', slope, 'score', score)\n",
    "\n",
    "print('min', _min, 'max', _max)\n",
    "\n",
    "# Colors\n",
    "\n",
    "cmap = plt.cm.RdYlBu_r\n",
    "cmap = plt.cm.coolwarm\n",
    "# norm = matplotlib.colors.Normalize(vmin=_min, vmax=_max)\n",
    "norm = matplotlib.colors.DivergingNorm(vmin=_min, vcenter=0, vmax=_max)\n",
    "# norm = matplotlib.colors.DivergingNorm(vmin=-10, vcenter=0, vmax=10)\n",
    "\n",
    "for slope in slopes:\n",
    "    color = cmap(norm(slope))\n",
    "    colors.append(color)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term Frequency Matrix\n",
    "\n",
    "import textacy\n",
    "\n",
    "doc_term_matrix, dictionary = textacy.representations.build_doc_term_matrix(texts, tf_type=\"linear\", idf_type=\"smooth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * creating mesh with size 40 40\n",
      " * filling mesh\n"
     ]
    }
   ],
   "source": [
    "# UMAP\n",
    "\n",
    "import umap\n",
    "from pointgrid import align_points_to_grid\n",
    "\n",
    "# random_state=2\n",
    "reducer = umap.UMAP(n_components=2, n_neighbors=2, min_dist=0.001, metric='cosine')\n",
    "# reducer = umap.UMAP(n_components=2, n_neighbors=2, metric='hellinger')\n",
    "\n",
    "embedding = reducer.fit_transform(doc_term_matrix)\n",
    "embedding = align_points_to_grid(embedding)\n",
    "x = embedding[:, 0]; y = embedding[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clustering on embedding\n",
    "\n",
    "import hdbscan\n",
    "\n",
    "# clusterer = hdbscan.HDBSCAN(min_cluster_size=4, min_samples=3, cluster_selection_epsilon=.5)\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=3, min_samples=2)\n",
    "# clusterer = hdbscan.HDBSCAN(cluster_selection_epsilon=0.3, cluster_selection_method='leaf')\n",
    "# min_samples is to consier all the elements that owtherwide will be classified as noise\n",
    "# cluster_selection_epsilon extends clusters\n",
    "clusterer.fit(embedding)\n",
    "clusters = clusterer.labels_\n",
    "\n",
    "# Grouping by cluster\n",
    "\n",
    "values = set(clusters)\n",
    "if -1 in values: values.remove(-1)\n",
    "\n",
    "clusters = [[index for index, cluster in enumerate(clusters) if cluster==value] for value in values]\n",
    "\n",
    "len(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy import interpolate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Frame\n",
    "\n",
    "plt.figure(figsize=(20,20), dpi=300)\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "# Hulls\n",
    "\n",
    "for cluster in clusters:\n",
    "\n",
    "    # Average color\n",
    "    \n",
    "    background_color = []\n",
    "    \n",
    "    for i, index in enumerate(cluster):\n",
    "        for occurence in range(occurences[index]):\n",
    "            background_color.append([colors[index][0], colors[index][1], colors[index][2]])\n",
    "\n",
    "    r = [i[0] for i in background_color]; r = sum(r) / len(r)\n",
    "    g = [i[1] for i in background_color]; g = sum(g) / len(g)\n",
    "    b = [i[2] for i in background_color]; b = sum(b) / len(b)\n",
    "\n",
    "    background_color = (r, g, b, 1)\n",
    "\n",
    "    # Hull\n",
    "\n",
    "    points = []\n",
    "    for index in cluster:\n",
    "        points.append([embedding[index][0], embedding[index][1]])\n",
    "    points = np.array(points)\n",
    "\n",
    "    # print(points)\n",
    "\n",
    "    hull = ConvexHull(points)\n",
    "    \n",
    "    x_hull = np.append(points[hull.vertices,0], points[hull.vertices,0][0]) # Collect the xs + first x\n",
    "    y_hull = np.append(points[hull.vertices,1], points[hull.vertices,1][0])\n",
    "\n",
    "    # print(x_hull)\n",
    "\n",
    "    # break\n",
    "    \n",
    "    # interpolate\n",
    "    dist = np.sqrt((x_hull[:-1] - x_hull[1:])**2 + (y_hull[:-1] - y_hull[1:])**2)\n",
    "    dist_along = np.concatenate(([0], dist.cumsum()))\n",
    "    spline, u = interpolate.splprep([x_hull, y_hull], u=dist_along, s=0)\n",
    "    interp_d = np.linspace(dist_along[0], dist_along[-1], 50)\n",
    "    interp_x, interp_y = interpolate.splev(interp_d, spline)\n",
    "    \n",
    "    # plot shape\n",
    "    plt.fill(interp_x, interp_y, '--', c=background_color, alpha=.2)\n",
    "\n",
    "\n",
    "# Scatterplot\n",
    "\n",
    "plt.scatter(x, y, s=occurences, c=colors)\n",
    "# plt.scatter(x, y, s=40, c=colors)\n",
    "\n",
    "\n",
    "# Labels\n",
    "\n",
    "for i, txt in enumerate(orgs):\n",
    "    # text = plt.annotate(orgs[i], xy=(x[i], y[i] - math.sqrt(occurences[i]/math.pi)/40), ha='center', va='bottom')\n",
    "    text = plt.annotate(orgs[i], xy=(x[i], y[i]), ha='center', va='bottom')\n",
    "    text.set_fontsize(3)\n",
    "\n",
    "plt.savefig('/Users/dario/Desktop/download.png')\n",
    "plt.savefig('download.png')\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c30f2af5f468e7f5b45bcc30fca5f4886c90d54777aed916ed5f6294dfb24bf2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
